stringsAsFactors = FALSE # Avoid converting strings to factors
)
View(video_df)
# Initialize variables with NA or default values
video_title <- NA
upload_date <- NA
views <- NA
likes <- NA
# Replace these with your actual video ID and API key
video_id <- "KkM93Y03Ip0"
api_key <- "AIzaSyBDbbHOpMaxXfQgX9FjqxnhSZKKsolarmY" # Make sure to replace with your actual API key
# API request for video metadata
response <- GET(paste0("https://www.googleapis.com/youtube/v3/videos?part=snippet,statistics&id=", video_id, "&key=", api_key))
metadata <- content(response, "parsed")
# Check if the API response contains items
if(length(metadata$items) > 0) {
video_title <- metadata$items[[1]]$snippet$title
upload_date <- metadata$items[[1]]$snippet$publishedAt
views <- metadata$items[[1]]$statistics$viewCount
likes <- metadata$items[[1]]$statistics$likeCount
}
# Create a dataframe
video_df <- data.frame(
Title = video_title,
UploadDate = upload_date,
Views = as.numeric(views),  # Ensuring numeric data is correctly typed
Likes = as.numeric(likes),
)
rm(list=ls)
rm(list=ls)
rm(list=ls())
# Initialize variables with NA or default values
video_title <- NA
upload_date <- NA
views <- NA
likes <- NA
# Replace these with your actual video ID and API key
video_id <- "KkM93Y03Ip0"
api_key <- "AIzaSyBDbbHOpMaxXfQgX9FjqxnhSZKKsolarmY" # Make sure to replace with your actual API key
# API request for video metadata
response <- GET(paste0("https://www.googleapis.com/youtube/v3/videos?part=snippet,statistics&id=", video_id, "&key=", api_key))
metadata <- content(response, "parsed")
# Check if the API response contains items
if(length(metadata$items) > 0) {
video_title <- metadata$items[[1]]$snippet$title
upload_date <- metadata$items[[1]]$snippet$publishedAt
views <- metadata$items[[1]]$statistics$viewCount
likes <- metadata$items[[1]]$statistics$likeCount
}
# Create a dataframe
video_df <- data.frame(
Title = video_title,
UploadDate = upload_date,
Views = as.numeric(views),  # Ensuring numeric data is correctly typed
Likes = as.numeric(likes)
)
# Initialize an empty data frame to store comments
comments_df <- data.frame(VideoID = character(), Comment = character(), stringsAsFactors = FALSE)
# Function to fetch comments for a given video ID
fetch_comments <- function(video_id, api_key) {
page_token <- ""  # For handling pagination
repeat {
# Construct API request URL, including page token if paginating
request_url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=", video_id, "&key=", api_key, "&textFormat=plainText", "&pageToken=", page_token)
response <- GET(request_url)
comments_data <- content(response, "parsed")
# Check for errors
if (!is.null(comments_data$error)) {
cat("Error fetching comments:", comments_data$error$message, "\n")
break
}
# Extract comments and add to data frame
for (item in comments_data$items) {
comment_text <- item$snippet$topLevelComment$snippet$textDisplay
comments_df <<- rbind(comments_df, data.frame(VideoID = video_id, Comment = comment_text, stringsAsFactors = FALSE))
}
# Check for next page token; if none, exit loop
page_token <- ifelse(is.null(comments_data$nextPageToken), "", comments_data$nextPageToken)
if (page_token == "") break
}
}
# Fetch comments for the specified video
fetch_comments(video_id, api_key)
# View the comments data frame
print(comments_df)
View(video_df)
View(comments_df)
rm(list=ls())
#### Code ####
get_video_ids_from_playlist <- function(playlist_id, api_key) {
video_ids <- c()
page_token <- ""
repeat {
response <- GET(paste0("https://www.googleapis.com/youtube/v3/playlistItems?part=snippet&maxResults=50&playlistId=", playlist_id, "&key=", api_key, "&pageToken=", page_token))
playlist_data <- content(response, "parsed")
if (!is.null(playlist_data$error)) {
cat("Error fetching playlist items:", playlist_data$error$message, "\n")
break
}
video_ids <- c(video_ids, sapply(playlist_data$items, function(item) item$snippet$resourceId$videoId))
page_token <- ifelse(is.null(playlist_data$nextPageToken), "", playlist_data$nextPageToken)
if (page_token == "") break
}
return(video_ids)
}
View(get_video_ids_from_playlist)
# Initialize data frames for storing video details and comments
video_details_df <- data.frame(VideoID = character(), Title = character(), UploadDate = character(), Views = numeric(), Likes = numeric(), stringsAsFactors = FALSE)
comments_df <- data.frame(VideoID = character(), Comment = character(), stringsAsFactors = FALSE)
# Replace with your actual playlist ID and API key
playlist_id <- "PLRnlRGar-_296KTsVL0R6MEbpwJzD8ppA"
api_key <- "AIzaSyBDbbHOpMaxXfQgX9FjqxnhSZKKsolarmY"
# Fetch video IDs from the playlist
video_ids <- get_video_ids_from_playlist(playlist_id, api_key)
# Loop through each video ID
for (i in seq_along(video_ids)) {
video_id <- video_ids[i]
# Print progress
cat("Processing video", i, "of", length(video_ids), "- Video ID:", video_id, "\n")
# Scrape video details (as previously defined)
response <- GET(paste0("https://www.googleapis.com/youtube/v3/videos?part=snippet,statistics&id=", video_id, "&key=", api_key))
metadata <- content(response, "parsed")
if (length(metadata$items) > 0) {
video_title <- metadata$items[[1]]$snippet$title
upload_date <- metadata$items[[1]]$snippet$publishedAt
views <- as.numeric(metadata$items[[1]]$statistics$viewCount)
likes <- as.numeric(metadata$items[[1]]$statistics$likeCount)
# Add video details to the dataframe
video_details_df <- rbind(video_details_df, data.frame(VideoID = video_id, Title = video_title, UploadDate = upload_date, Views = views, Likes = likes, stringsAsFactors = FALSE))
} else {
cat("No data found for video ID:", video_id, "\n")
}
# Scrape comments for the video (ensure fetch_comments function does not reinitialize comments_df each time)
fetch_comments(video_id, api_key)
# Optional: Add a short pause to avoid hitting rate limits too quickly
Sys.sleep(1)
}
fetch_comments <- function(video_id, api_key) {
page_token <- ""  # For handling pagination
repeat {
# Construct API request URL, including page token if paginating
request_url <- paste0("https://www.googleapis.com/youtube/v3/commentThreads?part=snippet&videoId=", video_id, "&key=", api_key, "&textFormat=plainText", "&pageToken=", page_token)
response <- GET(request_url)
comments_data <- content(response, "parsed")
# Check for errors
if (!is.null(comments_data$error)) {
cat("Error fetching comments:", comments_data$error$message, "\n")
break
}
# Extract comments and add to data frame
for (item in comments_data$items) {
comment_text <- item$snippet$topLevelComment$snippet$textDisplay
comments_df <<- rbind(comments_df, data.frame(VideoID = video_id, Comment = comment_text, stringsAsFactors = FALSE))
}
# Check for next page token; if none, exit loop
page_token <- ifelse(is.null(comments_data$nextPageToken), "", comments_data$nextPageToken)
if (page_token == "") break
}
}
# Initialize data frames for storing video details and comments
video_details_df <- data.frame(VideoID = character(), Title = character(), UploadDate = character(), Views = numeric(), Likes = numeric(), stringsAsFactors = FALSE)
comments_df <- data.frame(VideoID = character(), Comment = character(), stringsAsFactors = FALSE)
# Replace with your actual playlist ID and API key
playlist_id <- "PLRnlRGar-_296KTsVL0R6MEbpwJzD8ppA"
api_key <- "AIzaSyBDbbHOpMaxXfQgX9FjqxnhSZKKsolarmY"
# Fetch video IDs from the playlist
video_ids <- get_video_ids_from_playlist(playlist_id, api_key)
# Loop through each video ID
for (i in seq_along(video_ids)) {
video_id <- video_ids[i]
# Print progress
cat("Processing video", i, "of", length(video_ids), "- Video ID:", video_id, "\n")
# Scrape video details (as previously defined)
response <- GET(paste0("https://www.googleapis.com/youtube/v3/videos?part=snippet,statistics&id=", video_id, "&key=", api_key))
metadata <- content(response, "parsed")
if (length(metadata$items) > 0) {
video_title <- metadata$items[[1]]$snippet$title
upload_date <- metadata$items[[1]]$snippet$publishedAt
views <- as.numeric(metadata$items[[1]]$statistics$viewCount)
likes <- as.numeric(metadata$items[[1]]$statistics$likeCount)
# Add video details to the dataframe
video_details_df <- rbind(video_details_df, data.frame(VideoID = video_id, Title = video_title, UploadDate = upload_date, Views = views, Likes = likes, stringsAsFactors = FALSE))
} else {
cat("No data found for video ID:", video_id, "\n")
}
# Scrape comments for the video (ensure fetch_comments function does not reinitialize comments_df each time)
fetch_comments(video_id, api_key)
# Optional: Add a short pause to avoid hitting rate limits too quickly
Sys.sleep(1)
}
knitr::opts_chunk$set(echo = TRUE)
# acquire data
dat <- read.csv("https://raw.githubusercontent.com/gedeck/practical-statistics-for-data-scientists/master/data/house_sales.csv",
sep = "\t")
row.names(dat) <- 1:nrow(dat)
# transform property type to factor
dat$FacPropertyType <- as.factor(dat$PropertyType)
# run interaction model
mod1 <- lm(AdjSalePrice ~ FacPropertyType * SqFtTotLiving, data = dat)
summary(mod1)
knitr::opts_chunk$set(echo = TRUE)
summary(mod1)
View(dat)
summary(mod1)
# Checking for assumption of constant variance in the error terms:
scatter.smooth(x = fitted(mod1),
y = resid(mod1),
lpars = list(col = "pink"))
bptest(mod1)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(car)
library(lmtest)
library(sandwich)
library(psych)
summary(mod1)
# Checking for assumption of constant variance in the error terms:
scatter.smooth(x = fitted(mod1),
y = resid(mod1),
lpars = list(col = "pink"))
bptest(mod1)
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
library(tidyverse)
library(car)
library(lmtest)
library(sandwich)
library(psych)
summary(mod1)
# Checking for assumption of constant variance in the error terms:
scatter.smooth(x = fitted(mod1),
y = resid(mod1),
lpars = list(col = "pink"))
bptest(mod1)
summary(mod1)
# Checking for assumption of constant variance in the error terms:
scatter.smooth(x = fitted(mod1),
y = resid(mod1),
lpars = list(col = "pink"))
bptest(mod1)
options(scipen = 999)
mod3 <- lm(AdjSalePrice ~ Bedrooms + Bathrooms + SqFtTotLiving, data = dat)
stargazer::stargazer(mod3)
summary(mod3)
View(dat)
cor.plot(Filter(is.numeric, dat))
View(dat)
# Save current graphical parameters to reset later
old.par <- par()
# Set the desired margins and text size
par(cex.axis=0.7, cex.lab=0.7, cex.main=0.7)
# Your cor.plot command
cor.plot(Filter(is.numeric, dat))
# Reset to old graphical parameters
par(old.par)
cor.plot(Filter(is.numeric, dat))
knitr::opts_chunk$set(echo = TRUE)
options(scipen = 999)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
library(tidyverse)
library(car)
library(lmtest)
library(sandwich)
library(psych)
cor.plot(Filter(is.numeric, dat))
png(filename = "cor_plot.png", width = 800, height = 600)
cor.plot(Filter(is.numeric, dat))
png(filename = "cor_plot.png", width = 800, height = 600)
cor.plot(Filter(is.numeric, dat))
png(filename = "cor_plot.png", width = 800, height = 600)
cor_matrix <- cor(Filter(is.numeric, dat))
View(cor_matrix)
View(dat)
View(dat)
# Removing objects
rm(list=ls())
# Setting wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Detaching all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# Loading libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
data(child)
lapply(c("survival", "eha", "tidyverse", "ggfortify", "stargazer"),  pkgTest)
data(child)
data(child)
child_surv <- with(child, Surv(enter, exit, event))
View(child)
cox <- coxph(child_surv ~ sex + m.age, data = child)
summary(cox)
drop1(cox, test = "Chisq")
# Hazard Ratio interpretation
exp(-0.083546) # Female vs. Male
exp(0.007617)  # Mom's age
# Plotting Cox PH model
cox_fit <- survfit(cox)
autoplot(cox_fit)
# Creating mom's age groups to plot survival curves
plot(child_surv$m.age)
# Creating mom's age groups to plot survival curves
plot(child$m.age)
# Creating mom's age groups to plot survival curves
plot(child$m.age)
plot(child$m.age)
child <- child %>%
mutate(m.age_groups = ifelse(m.age <= 25, "25 or younger",
ifelse(m.age <= 30, "25 to 30 y.o.",
ifelse(m.age <= 35, "30 to 25 y.o.",
"35 or older"))))
# Plotting survival curves by sex and mom's age
newdat <- expand.grid(sex = c("male", "female"),
m.age_groups = unique(child$m.age_groups))
plot(survfit(cox, newdata = newdat), xscale = 12,
conf.int = T,
ylim = c(0.6, 1),
col = c("red", "blue"),
xlab = "Time",
ylab = "Survival proportion",
main = "")
child <- child %>%
mutate(m.age_groups = ifelse(m.age <= 25, "25 or younger",
ifelse(m.age <= 30, "25 to 30 y.o.",
ifelse(m.age <= 35, "30 to 25 y.o.",
"35 or older"))))
View(child)
# Plotting survival curves by sex and mother's age groups
newdat <- expand.grid(sex = c("male", "female"),
m.age_groups = unique(child$m.age_groups))
plot(survfit(cox, newdata = newdat), xscale = 12,
conf.int = TRUE,
ylim = c(0.6, 1),
col = c("red", "blue"),
xlab = "Time",
ylab = "Survival proportion",
main = "Survival Curves by Sex and Mother's Age Group")
# Fitting Cox PH model without interaction
cox <- coxph(child_surv ~ sex + m.age, data = child)
summary(cox)
drop1(cox, test = "Chisq")
# Hazard Ratio interpretation
exp(-0.083546) # Female vs. Male
exp(0.007617)  # Mom's age
# Plotting Cox PH model
cox_fit <- survfit(cox)
autoplot(cox_fit)
# Creating mom's age groups to plot survival curves
plot(child$m.age)
child <- child %>%
mutate(m.age_groups = ifelse(m.age <= 25, "25 or younger",
ifelse(m.age <= 30, "25 to 30 y.o.",
ifelse(m.age <= 35, "30 to 25 y.o.",
"35 or older"))))
# Plotting survival curves by sex and mother's age groups
newdat <- expand.grid(sex = c("male", "female"),
m.age_groups = unique(child$m.age_groups))
plot(survfit(cox, newdata = newdat), xscale = 12,
conf.int = TRUE,
ylim = c(0.6, 1),
col = c("red", "blue"),
xlab = "Time",
ylab = "Survival proportion",
main = "Survival Curves by Sex and Mother's Age Group")
# Plotting survival curves by sex and mother's age groups
plot(survfit(cox, newdata = newdat), xscale = 12,
conf.int = TRUE,
ylim = c(0.6, 1),
col = c("red", "blue"),
xlab = "Time",
ylab = "Survival proportion",
main = "Survival Curves by Sex and Mother's Age Group")
# Plotting survival curves by sex and mother's age groups
newdat <- expand.grid(sex = c("male", "female"),
m.age_groups = unique(child$m.age_groups))
plot(survfit(cox, newdata = newdat), xscale = 12,
conf.int = TRUE,
ylim = c(0.6, 1),
col = c("red", "blue"),
xlab = "Time",
ylab = "Survival proportion",
main = "Survival Curves by Sex and Mother's Age Group")
# Fitting Cox PH model without interaction
cox <- coxph(child_surv ~ sex + m.age, data = child)
summary(cox)
drop1(cox, test = "Chisq")
# Hazard Ratio interpretation
exp(-0.083546) # Female vs. Male
exp(0.007617)  # Mom's age
# Plotting Cox PH model
cox_fit <- survfit(cox)
autoplot(cox_fit)
# Creating mom's age groups to plot survival curves
plot(child$m.age)
child <- child %>%
mutate(m.age_groups = ifelse(m.age <= 25, "25 or younger",
ifelse(m.age <= 30, "25 to 30 y.o.",
ifelse(m.age <= 35, "30 to 25 y.o.",
"35 or older"))))
# Plotting survival curves by sex and mother's age groups
newdat <- expand.grid(sex = c("male", "female"),
m.age_groups = unique(child$m.age_groups))
plot(survfit(cox, newdata = newdat), xscale = 12,
conf.int = TRUE,
ylim = c(0.6, 1),
col = c("red", "blue"),
xlab = "Time",
ylab = "Survival proportion",
main = "Survival Curves by Sex and Mother's Age Group")
# remove objects
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# here is where you load any necessary packages
# ex: stringr
# lapply(c("stringr"),  pkgTest)
lapply(c("survival", "eha", "tidyverse", "ggfortify", "stargazer"),  pkgTest)
data(child)
#### Completed
# a)
child_surv <- with(child, Surv(enter, exit, event))
km <- survfit(child_surv ~ 1, data = child)
summary(km, times = seq(0, 15, 1))
plot(km, main = "Kaplan-Meier Plot", xlab = "Years", ylim = c(0.7, 1))
autoplot(km)
km_socBranch <- survfit(child_surv ~ socBranch, data = child)
summary (km_socBranch)
autoplot(km_socBranch)
# b)
cox <- coxph(child_surv ~ sex + socBranch, data = child)
summary(cox)
drop1(cox, test = "Chisq")
stargazer(cox, type = "text")
# Plotting Cox PH model
cox_fit <- survfit(cox)
autoplot(cox_fit)
# By gender:
newdat <- data.frame(sex = c("male", "female"))
# Generate survival curves for the new data
surv_curves <- survfit(cox, newdata = newdat)
#### Libraries and options ####
# Removing objects
rm(list=ls())
# Setting wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Detaching all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# Loading libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
lapply(c("survival", "eha", "tidyverse", "ggfortify", "stargazer"),  pkgTest)
#### Data and survival object ####
data(child)
child_surv <- with(child, Surv(enter, exit, event))
#### Kaplan-Meier and Survival Plots ####
# Kaplan-Meier estimation
km <- survfit(child_surv ~ 1, data = child)
summary(km, times = seq(0, 15, 1))
plot(km, main = "Kaplan-Meier Plot", xlab = "Years", ylim = c(0.7, 1))
autoplot(km)
# Kaplan-Meier estimation by socio-economic branch
km_socBranch <- survfit(child_surv ~ socBranch, data = child)
summary(km_socBranch)
autoplot(km_socBranch)
#### Cox Proportional Hazard Model ####
# Fitting the Cox PH model:
cox <- coxph(child_surv ~ sex + m.age, data = child)
summary(cox)
drop1(cox, test = "Chisq")
# Exponantiating Hazard Ratio for Interpretation
exp(-0.083546) # Female vs. Male
exp(0.007617)  # Mom's age
# Plotting Cox PH model
cox_fit <- survfit(cox)
autoplot(cox_fit)
# Plotting survival curves by groups:
# By gender:
newdat <- data.frame(sex = c("male", "female"))
# Generate survival curves for the new data
surv_curves <- survfit(cox, newdata = newdat)
