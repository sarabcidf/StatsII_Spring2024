ggplot(aes(x = reorder(feature, frequency), y = frequency)) +
geom_point() +
coord_flip() +
labs(x = "feature")
# We can also bind together our three dfms
dfm_ukr <- rbind(dfm22, dfm23, dfm24)
# This allows us to analyse and compare keyness across years
set.seed(2023)
dfm_by_date <- dfm_group(dfm_ukr, fill = TRUE, groups = year(dfm_ukr$date))
keyness <- textstat_keyness(dfm_by_date, target = "2022")
textplot_keyness(keyness, labelsize = 3)
keyness <- textstat_keyness(dfm_by_date, target = "2024")
textplot_keyness(keyness, labelsize = 3)
# Try changing this code to compare keyness of 2024 against the previous two years
# Finally, let's see if sentiment analysis can detect a change in tone
# over time. To do this, we need to use a dictionary object. We only
# want positive and negative sentiments, so we'll just use the first two
# elements.
dfm_sentiment <- dfm_lookup(dfm_ukr, data_dictionary_LSD2015[1:2]) %>%
dfm_group(groups = date)
# Once we have the frequency for positive and negative sentiment, we can
# use a useful feature of R - vectorisation - to calculate net sentiment
# across each day and plot it.
docvars(dfm_sentiment, "prop_negative") <- as.numeric(dfm_sentiment[,1] / ntoken(dfm_sentiment))
docvars(dfm_sentiment, "prop_positive") <- as.numeric(dfm_sentiment[,2] / ntoken(dfm_sentiment))
docvars(dfm_sentiment, "net_sentiment") <- docvars(dfm_sentiment, "prop_positive") - docvars(dfm_sentiment,"prop_negative")
docvars(dfm_sentiment) %>%
ggplot(aes(x = yday(date), y = net_sentiment, group = year(date))) +
geom_smooth(aes(colour = as.character(year(date)))) +
labs(title = "Sentiment over time",
x = "day of year", y = "net sentiment",
colour = "year")
# Having performed these analyses, is there anything you would change in
# the initial corpus? Try changing a few things and see how it affects
# the results.
keyness <- textstat_keyness(dfm_by_date, target = "2024")
textplot_keyness(keyness, labelsize = 3)
# save our data for next time
saveRDS(dfm22, "/Users/sarabcidf/Desktop/ASDS/Text/QTA_Spring23/tutorials/tutorial03/data/dfm22")
saveRDS(dfm23, "/Users/sarabcidf/Desktop/ASDS/Text/QTA_Spring23/tutorials/tutorial03/data/dfm23")
saveRDS(dfm24, "/Users/sarabcidf/Desktop/ASDS/Text/QTA_Spring23/tutorials/tutorial03/data/dfm24")
keyness <- textstat_keyness(dfm_by_date, target = "2022")
textplot_keyness(keyness, labelsize = 3)
keyness <- textstat_keyness(dfm_by_date, target = "2024")
textplot_keyness(keyness, labelsize = 3)
dfm_sentiment <- dfm_lookup(dfm_ukr, data_dictionary_LSD2015[1:2]) %>%
dfm_group(groups = date)
docvars(dfm_sentiment, "prop_negative") <- as.numeric(dfm_sentiment[,1] / ntoken(dfm_sentiment))
docvars(dfm_sentiment, "prop_positive") <- as.numeric(dfm_sentiment[,2] / ntoken(dfm_sentiment))
docvars(dfm_sentiment, "net_sentiment") <- docvars(dfm_sentiment, "prop_positive") - docvars(dfm_sentiment,"prop_negative")
docvars(dfm_sentiment) %>%
ggplot(aes(x = yday(date), y = net_sentiment, group = year(date))) +
geom_smooth(aes(colour = as.character(year(date)))) +
labs(title = "Sentiment over time",
x = "day of year", y = "net sentiment",
colour = "year")
getwd()
getwd()
setwd("/Users/sarabcidf/Desktop")
rm(ls=())
rm(ls = ())
rm(ls=list())
rm(ls=list())
rm(list=ls())
getwd()
read_csv("/Users/sarabcidf/Desktop/2023-opps.csv")
cim <- read_csv("/Users/sarabcidf/Desktop/2023-opps.csv")
View(cim)
library(tidyverse)
cim <- cim %>%
rename_with(~ str_replace_all(str_to_lower(.), " ", ""))
View(cim)
count(cim, jobrole)
cim %>%
group_by(jobrole, stage)
count(cim, jobrole, stage)
count(cim, stage)
count(cim, jobrole)
count(cim, stage)
cim %>%
group_by(jobrole, stage) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(percentage = count / sum(count) * 100) %
cim %>%
group_by(jobrole, stage) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(percentage = count / sum(count) * 100) %>%
arrange(jobrole, desc(percentage))
chart <- cim %>%
group_by(jobrole, stage) %>%
summarise(count = n(), .groups = 'drop') %>%
mutate(percentage = count / sum(count) * 100) %>%
arrange(jobrole, desc(percentage))
View(chart)
chart <- cim %>%
group_by(jobrole, stage) %>%
summarise(count = n(), .groups = 'drop') %>%
group_by(jobrole) %>% # Re-group by jobrole to calculate percentages within each job role
mutate(percentage = count / sum(count) * 100) %>% # Calculate percentage within each job role
ungroup() %>% # Remove the grouping
arrange(jobrole, desc(percentage))
View(chart)
count(cim, stage)
cim <- cim %>%
mutate(stage = recode(stage,
"Closed Secured" = "Closed Won"))
count(cim, stage)
count(cim, jobrole)
cim <- cim %>%
mutate(jobrole = recode(jobrole,
"Energy" = "Energy or Sustainability",
"Sustainability" = "Energy or Sustainability",
"Senior Facility" = "Senior Facility or Operations",
"Operations" = "Senior Facility or Operations"))
count(cim, jobrole)
chart <- cim %>%
group_by(jobrole, stage) %>%
summarise(count = n(), .groups = 'drop') %>%
group_by(jobrole) %>%
mutate(percentage = count / sum(count) * 100) %>%
ungroup() %>%
arrange(jobrole, desc(percentage)) %>%
ungroup()
ggplot(chart, aes(x = stage, y = percentage, fill = stage)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~ jobrole, scales = "free_x") +
labs(title = "Deal Stages by Job Role", x = "Stage", y = "Percentage") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
ggplot(chart, aes(x = stage, y = percentage, fill = stage)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~ jobrole, scales = "free_x") +
labs(title = "Deal Stages by Job Role", x = "Stage", y = "Percentage") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
geom_text(aes(label = sprintf("%.1f%%", percentage)), # Format the label as a percentage
position = position_dodge(width = 0.9), # Adjust position to align with dodged bars
vjust = -0.5, # Adjust vertical position to be just above the bars
size = 3))
ggplot(chart, aes(x = stage, y = percentage, fill = stage)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~ jobrole, scales = "free_x") +
labs(title = "Deal Stages by Job Role", x = "Stage", y = "Percentage") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
geom_text(aes(label = sprintf("%.1f%%", percentage)), # Format the label as a percentage
position = position_dodge(width = 0.9), # Adjust position to align with dodged bars
vjust = -0.5, # Adjust vertical position to be just above the bars
size = 3)
ggplot(chart, aes(x = stage, y = percentage, fill = stage)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~ jobrole, scales = "free_x", ncol = 1) +
labs(title = "Deal Stages by Job Role", x = "Stage", y = "Percentage") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
geom_text(aes(label = sprintf("%.1f%%", percentage)),
position = position_dodge(width = 0.9),
vjust = -0.5,
size = 3)
ggplot(chart, aes(x = stage, y = percentage, fill = stage)) +
geom_bar(stat = "identity", position = "dodge") +
facet_wrap(~ jobrole, scales = "free_x") +
labs(title = "Deal Stages by Job Role", x = "Stage", y = "Percentage") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
geom_text(aes(label = sprintf("%.1f%%", percentage)),
position = position_dodge(width = 0.9),
vjust = -0.5,
size = 3)
View(chart)
observations_per_role <- cim %>%
count(jobrole) %>%
rename(total = n)
# Merge this information back into df_prepared if it doesn't already contain it
chart <- chart %>%
left_join(observations_per_role, by = "jobrole")
# Get unique job roles
unique_roles <- unique(df_prepared$jobrole)
observations_per_role <- cim %>%
count(jobrole) %>%
rename(total = n)
# Merge this information back into df_prepared if it doesn't already contain it
chart <- chart %>%
left_join(observations_per_role, by = "jobrole")
# Get unique job roles
unique_roles <- unique(chart$jobrole)
# Calculate the number of observations per job role
observations_per_role <- cim %>%
count(jobrole) %>%
rename(total = n)
# Merge this information back into df_prepared if it doesn't already contain it
chart <- chart %>%
left_join(observations_per_role, by = "jobrole")
# Get unique job roles
unique_roles <- unique(chart$jobrole)
# Loop through each unique job role and create a plot, including the observation count
plots <- list() # Initialize an empty list to store plots
for (role in unique_roles) {
# Filter the data for the current job role
df_role <- chart %>% filter(jobrole == role)
# Get the number of observations for the current job role
num_observations <- df_role$total[1] # Assuming total is the same for all rows of the same job role
# Create the plot for the current job role
p <- ggplot(df_role, aes(x = stage, y = percentage, fill = stage)) +
geom_bar(stat = "identity", position = "dodge") +
geom_text(aes(label = sprintf("%.1f%%", percentage)),
position = position_dodge(width = 0.9),
vjust = -0.5,
size = 3) +
labs(title = paste("Deal Stages for", role, "\nTotal Deals:", num_observations),
x = "Stage",
y = "Percentage") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
# Add the plot to the list
plots[[role]] <- p
}
# Example of how to view or save a plot for a specific job role
# print(plots[["Role A"]]) # Replace "Role A" with the actual job role name
# To save plots to files, loop through the `plots` list
for (role in names(plots)) {
ggsave(paste("plot_", role, ".png", sep = ""), plots[[role]], width = 10, height = 8)
}
rm(list=ls())
rm(list=ls())
# detach all libraries
detachAllPackages <- function() {
basic.packages <- c("package:stats", "package:graphics", "package:grDevices", "package:utils", "package:datasets", "package:methods", "package:base")
package.list <- search()[ifelse(unlist(gregexpr("package:", search()))==1, TRUE, FALSE)]
package.list <- setdiff(package.list, basic.packages)
if (length(package.list)>0)  for (package in package.list) detach(package,  character.only=TRUE)
}
detachAllPackages()
# load libraries
pkgTest <- function(pkg){
new.pkg <- pkg[!(pkg %in% installed.packages()[,  "Package"])]
if (length(new.pkg))
install.packages(new.pkg,  dependencies = TRUE)
sapply(pkg,  require,  character.only = TRUE)
}
# set wd for current folder
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
lapply(c("tidyverse"),  pkgTest)
set.seed(123)
data <- rcauchy(1000)
# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
print(D)
kol_smirn_p <- function(D) {
# Creating constants
sqrtpi = sqrt(2 * pi)
# Initialiting variables
sum_serie = 0
# Summing from k = 1 to 1000
for (k in 1:1000) {
term = exp(-(2*k - 1)^2 * pi^2 / (8 * D^2))
sum_serie = sum_serie + term
}
p_val = (pi_sqrt / D) * sum_serie
return(p_val)
}
# Substiting our D test statistic value:
kol_smirn_p(D)
kol_smirn_p <- function(D) {
# Creating constants
sqrtpi = sqrt(2 * pi)
# Initialiting variables
sum_serie = 0
# Summing from k = 1 to 1000
for (k in 1:1000) {
term = exp(-(2*k - 1)^2 * pi^2 / (8 * D^2))
sum_serie = sum_serie + term
}
p_val = (sqrtpi / D) * sum_serie
return(p_val)
}
# Substiting our D test statistic value:
kol_smirn_p(D)
pvalue <- kol_smirn_p(D)
print(pvalue)
lm_mod <- lm(y ~ x, data = data)
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
rss_function <- function(params, data) {
# Extracting parameters
intercept <- params[1]
slope <- params[2]
# Calculating residuals
residuals <- data$y - (intercept + slope * data$x)
# Calculating RSS
rss <- sum(residuals^2)
return(rss)
}
# Initial guesses for parameters
initial_params <- c(intercept = 0, slope = 1)
# Optimization using BFGS
optim_result <- optim(par = initial_params, fn = rss_function, data = data, method = "BFGS")
# Extract optimized parameters
optim_result$par
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
neg_log_likelihood <- function(params, data) {
beta0 <- params[1]
beta1 <- params[2]
sigma2 <- params[3]
n <- nrow(data)
residuals <- data$y - (beta0 + beta1 * data$x)
nll <- (n/2) * log(sigma2) + sum(residuals^2) / (2 * sigma2)
return(nll)
}
# Initial parameter guesses
initial_params <- c(beta0 = 0, beta1 = 1, sigma2 = 1)
# Minimize the negative log-likelihood
optim_result <- optim(par = initial_params, fn = neg_log_likelihood, data = data, method = "BFGS")
# Display the optimized parameters
optim_result$par
?optim
log_likelihood <- function(params, data) {
beta0 <- params[1]
beta1 <- params[2]
sigma2 <- abs(params[3])  # Ensure variance is positive
n <- nrow(data)
residuals <- data$y - (beta0 + beta1 * data$x)
ll <- -(n/2) * log(2 * pi * sigma2) - sum(residuals^2) / (2 * sigma2)
return(ll)
}
# Initial parameter guesses
initial_params <- c(beta0 = 0, beta1 = 1, sigma2 = 1)
# Maximize the log-likelihood using BFGS method
mle <- optim(par = initial_params, fn = log_likelihood, data = data, method = "BFGS", control = list(fnscale = -1))
# Extract MLE parameters
mle$par
log_likelihood <- function(params, data) {
alpha <- params[1]
beta1 <- params[2]
sig2 <- abs(params[3])  # Ensure variance is positive
n <- nrow(data)
residuals <- data$y - (alpha + beta1 * data$x)
loglike <- -(n/2) * log(2 * pi * sig2) - sum(residuals^2) / (2 * sig2)
return(loglike)
}
# Setting initial parameter values to start optimization
initial_params <- c(alpha = 0, beta1 = 1, sig2 = 1)
# Maximizing the log-likelihood using BFGS method
?optim # Optim minimizes by default but we can change it
mle <- optim(par = initial_params, fn = log_likelihood, data = data, method = "BFGS", control = list(fnscale = -1))
# Extract MLE parameters
mle$par
# Comparing with lm:
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
summary(mle)
mle$par
# Seeing results:
mle$par
summary(lm_mod)
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize
# My parameters for OLS are the intercept, the slope and the variance of the
# error term (sigma squared)
# Then I take the formula of the log likelihood for a normal distribution
# This is based on the slides for class:
linear.lik <- function(theta, y, X) {
n <- nrow(X) # Number of observations
k <- ncol(X) # Number of parameters (including intercept)
beta <- theta[1:k] # Regression coefficients
sigma2 <- theta[k+1]^2 # Variance of errors, ensuring it's positive
e <- y - X %*% beta # Calculate residuals
logl <- -.5 * n * log(2 * pi) - .5 * n * log(sigma2) - (t(e) %*% e) / (2 * sigma2)
return(-logl) # Return negative log-likelihood for minimization
}
# Maximizing the log-likelihood using BFGS method
?optim # Optim minimizes by default but we can change it
# This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data$y, X = cbind(1, data$x), method = "BFGS")
# Seeing results:
mle$par
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
View(data)
linear.lik <- function(theta, y, X) {
n <- nrow(X) # Number of observations
k <- ncol(X) # Number of parameters (including intercept)
beta <- theta[1:k] # Regression coefficients
log_sigma2 <- theta[k+1]  # Work with log(sigma^2)
sigma2 <- exp(log_sigma2)  # Transform back to ensure positivity
e <- y - X %*% beta # Calculate residuals
logl <- -.5 * n * log(2 * pi) - .5 * n * log(sigma2) - (t(e) %*% e) / (2 * sigma2)
return(-logl) # Return negative log-likelihood for minimization
}
# Maximizing the log-likelihood using BFGS method
?optim # Optim minimizes by default but we can change it
# This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data$y, X = cbind(1, data$x), method = "BFGS")
# Seeing results:
mle$par
# Seeing results:
linear.MLE$par
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize
# My parameters for OLS are the intercept, the slope and the variance of the
# error term (sigma squared)
# Then I take the formula of the log likelihood for a normal distribution
# This is based on the slides for class:
linear.lik <- function(theta, y, X) {
n <- nrow(X) # Number of obs
k <- ncol(X) # Number of parameters to estimate
beta <- theta[1:k] # Regression coefficients
sigma2 <- theta[k+1]^2 # Variance of errors
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
# Maximizing the log-likelihood using BFGS method
# This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data$y, X = cbind(1, data$x), method = "BFGS")
# Seeing results:
linear.MLE$par
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize
# My parameters for OLS are the intercept, the slope and the variance of the
# error term (sigma squared)
# Then I take the formula of the log likelihood for a normal distribution
# This is based on the slides for class:
linear.lik <- function(theta, y, X) {
n <- nrow(X) # Number of obs
k <- ncol(X) # Number of parameters to estimate
beta <- theta[1:k] # Regression coefficients
sigma2 <- theta[k+1]^2 # Variance of errors
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
# Maximizing the log-likelihood (or minimizing negative log likelihood)
# using BFGS method. This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data$y, X = cbind(1, data$x), method = "BFGS")
# Seeing results:
linear.MLE$par
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X){ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2 e <- y - X%*%beta
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X){ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear .MLE <- optim(fn=linear . lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = " BFGS" )
linear .MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = " BFGS" )
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = " BFGS" )
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X){ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X)
{ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
rm(list=ls())
rm(list=ls())
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X)
{ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
