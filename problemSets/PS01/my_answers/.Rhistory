# create empirical distribution of observed data
ECDF <- ecdf(data)
empiricalCDF <- ECDF(data)
# generate test statistic
D <- max(abs(empiricalCDF - pnorm(data)))
print(D)
kol_smirn_p <- function(D) {
# Creating constants
sqrtpi = sqrt(2 * pi)
# Initialiting variables
sum_serie = 0
# Summing from k = 1 to 1000
for (k in 1:1000) {
term = exp(-(2*k - 1)^2 * pi^2 / (8 * D^2))
sum_serie = sum_serie + term
}
p_val = (pi_sqrt / D) * sum_serie
return(p_val)
}
# Substiting our D test statistic value:
kol_smirn_p(D)
kol_smirn_p <- function(D) {
# Creating constants
sqrtpi = sqrt(2 * pi)
# Initialiting variables
sum_serie = 0
# Summing from k = 1 to 1000
for (k in 1:1000) {
term = exp(-(2*k - 1)^2 * pi^2 / (8 * D^2))
sum_serie = sum_serie + term
}
p_val = (sqrtpi / D) * sum_serie
return(p_val)
}
# Substiting our D test statistic value:
kol_smirn_p(D)
pvalue <- kol_smirn_p(D)
print(pvalue)
lm_mod <- lm(y ~ x, data = data)
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
rss_function <- function(params, data) {
# Extracting parameters
intercept <- params[1]
slope <- params[2]
# Calculating residuals
residuals <- data$y - (intercept + slope * data$x)
# Calculating RSS
rss <- sum(residuals^2)
return(rss)
}
# Initial guesses for parameters
initial_params <- c(intercept = 0, slope = 1)
# Optimization using BFGS
optim_result <- optim(par = initial_params, fn = rss_function, data = data, method = "BFGS")
# Extract optimized parameters
optim_result$par
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
neg_log_likelihood <- function(params, data) {
beta0 <- params[1]
beta1 <- params[2]
sigma2 <- params[3]
n <- nrow(data)
residuals <- data$y - (beta0 + beta1 * data$x)
nll <- (n/2) * log(sigma2) + sum(residuals^2) / (2 * sigma2)
return(nll)
}
# Initial parameter guesses
initial_params <- c(beta0 = 0, beta1 = 1, sigma2 = 1)
# Minimize the negative log-likelihood
optim_result <- optim(par = initial_params, fn = neg_log_likelihood, data = data, method = "BFGS")
# Display the optimized parameters
optim_result$par
?optim
log_likelihood <- function(params, data) {
beta0 <- params[1]
beta1 <- params[2]
sigma2 <- abs(params[3])  # Ensure variance is positive
n <- nrow(data)
residuals <- data$y - (beta0 + beta1 * data$x)
ll <- -(n/2) * log(2 * pi * sigma2) - sum(residuals^2) / (2 * sigma2)
return(ll)
}
# Initial parameter guesses
initial_params <- c(beta0 = 0, beta1 = 1, sigma2 = 1)
# Maximize the log-likelihood using BFGS method
mle <- optim(par = initial_params, fn = log_likelihood, data = data, method = "BFGS", control = list(fnscale = -1))
# Extract MLE parameters
mle$par
log_likelihood <- function(params, data) {
alpha <- params[1]
beta1 <- params[2]
sig2 <- abs(params[3])  # Ensure variance is positive
n <- nrow(data)
residuals <- data$y - (alpha + beta1 * data$x)
loglike <- -(n/2) * log(2 * pi * sig2) - sum(residuals^2) / (2 * sig2)
return(loglike)
}
# Setting initial parameter values to start optimization
initial_params <- c(alpha = 0, beta1 = 1, sig2 = 1)
# Maximizing the log-likelihood using BFGS method
?optim # Optim minimizes by default but we can change it
mle <- optim(par = initial_params, fn = log_likelihood, data = data, method = "BFGS", control = list(fnscale = -1))
# Extract MLE parameters
mle$par
# Comparing with lm:
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
summary(mle)
mle$par
# Seeing results:
mle$par
summary(lm_mod)
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize
# My parameters for OLS are the intercept, the slope and the variance of the
# error term (sigma squared)
# Then I take the formula of the log likelihood for a normal distribution
# This is based on the slides for class:
linear.lik <- function(theta, y, X) {
n <- nrow(X) # Number of observations
k <- ncol(X) # Number of parameters (including intercept)
beta <- theta[1:k] # Regression coefficients
sigma2 <- theta[k+1]^2 # Variance of errors, ensuring it's positive
e <- y - X %*% beta # Calculate residuals
logl <- -.5 * n * log(2 * pi) - .5 * n * log(sigma2) - (t(e) %*% e) / (2 * sigma2)
return(-logl) # Return negative log-likelihood for minimization
}
# Maximizing the log-likelihood using BFGS method
?optim # Optim minimizes by default but we can change it
# This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data$y, X = cbind(1, data$x), method = "BFGS")
# Seeing results:
mle$par
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
View(data)
linear.lik <- function(theta, y, X) {
n <- nrow(X) # Number of observations
k <- ncol(X) # Number of parameters (including intercept)
beta <- theta[1:k] # Regression coefficients
log_sigma2 <- theta[k+1]  # Work with log(sigma^2)
sigma2 <- exp(log_sigma2)  # Transform back to ensure positivity
e <- y - X %*% beta # Calculate residuals
logl <- -.5 * n * log(2 * pi) - .5 * n * log(sigma2) - (t(e) %*% e) / (2 * sigma2)
return(-logl) # Return negative log-likelihood for minimization
}
# Maximizing the log-likelihood using BFGS method
?optim # Optim minimizes by default but we can change it
# This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data$y, X = cbind(1, data$x), method = "BFGS")
# Seeing results:
mle$par
# Seeing results:
linear.MLE$par
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize
# My parameters for OLS are the intercept, the slope and the variance of the
# error term (sigma squared)
# Then I take the formula of the log likelihood for a normal distribution
# This is based on the slides for class:
linear.lik <- function(theta, y, X) {
n <- nrow(X) # Number of obs
k <- ncol(X) # Number of parameters to estimate
beta <- theta[1:k] # Regression coefficients
sigma2 <- theta[k+1]^2 # Variance of errors
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
# Maximizing the log-likelihood using BFGS method
# This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data$y, X = cbind(1, data$x), method = "BFGS")
# Seeing results:
linear.MLE$par
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize
# My parameters for OLS are the intercept, the slope and the variance of the
# error term (sigma squared)
# Then I take the formula of the log likelihood for a normal distribution
# This is based on the slides for class:
linear.lik <- function(theta, y, X) {
n <- nrow(X) # Number of obs
k <- ncol(X) # Number of parameters to estimate
beta <- theta[1:k] # Regression coefficients
sigma2 <- theta[k+1]^2 # Variance of errors
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
# Maximizing the log-likelihood (or minimizing negative log likelihood)
# using BFGS method. This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data$y, X = cbind(1, data$x), method = "BFGS")
# Seeing results:
linear.MLE$par
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X){ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2 e <- y - X%*%beta
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X){ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear .MLE <- optim(fn=linear . lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = " BFGS" )
linear .MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = " BFGS" )
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = " BFGS" )
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X){ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X)
{ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
rm(list=ls())
rm(list=ls())
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X)
{ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
rm(list=ls())
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X)
{ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
rm(list=ls())
set.seed (2024) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X)
{ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
rm(list=ls())
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X)
{ n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k+1]^2
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize
# My parameters for OLS are the intercept, the slope and the variance of the
# error term (sigma squared)
# Then I take the formula of the log likelihood for a normal distribution
# This is based on the slides for class:
linear.lik <- function(theta, y, X) {
n <- nrow(X) # Number of obs
k <- ncol(X) # Number of parameters to estimate
beta <- theta[1:k] # Regression coefficients
sigma2 <- theta[k+1]^2 # Variance of errors
e <- y - X%*%beta
logl <- -.5*n*log(2*pi) -.5*n*log(sigma2) - ( (t(e) %*% e)/ (2*sigma2) )
return ( - logl ) }
# Maximizing the log-likelihood (or minimizing negative log likelihood)
# using BFGS method. This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data$y, X = cbind(1, data$x), method = "BFGS")
# Seeing results:
linear.MLE$par
# Comparing with lm:
lm_mod <- lm(y ~ x, data = data)
summary(lm_mod)
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize
# My parameters for OLS are the intercept, the slope and the variance of the
# error term (sigma squared)
# Then I take the formula of the log likelihood for a normal distribution
log_likelihood <- function(params, data) {
alpha <- params[1]
beta1 <- params[2]
sig2 <- abs(params[3])  # Ensure variance is positive
n <- nrow(data)
residuals <- data$y - (alpha + beta1 * data$x)
loglike <- -(n/2) * log(2 * pi * sig2) - sum(residuals^2) / (2 * sig2)
return(loglike)
}
# Setting initial parameter values to start optimization
initial_params <- c(alpha = 0, beta1 = 1, sig2 = 1)
# Maximizing the log-likelihood using BFGS method
?optim # Optim minimizes by default but we can change it
# This code is adapted from slides:
mle <- optim(par = initial_params, fn = log_likelihood, data = data, method = "BFGS", control = list(fnscale = -1))
# Seeing results:
mle$par
rm(list=ls())
set.seed (123) # Setting my seed
data <- data.frame(x = runif(200, 1, 10)) # Creating new data
data$y <- 0 + 2.75*data$x + rnorm(200, 0, 1.5) # Modeling relationship
linear.lik <- function(theta, y, X) {
n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k + 1]^2
e <- y - X %*% beta
logl <- -0.5 * n * log(2 * pi) - 0.5 * n * log(sigma2) - ((t(e) %*% e) / (2 * sigma2))
return(-logl)
}
linear.MLE <- optim(fn=linear.lik , par=c(1 ,1 ,1) , hessian= TRUE, y=data$y, X=cbind(1, data$x), method = "BFGS" )
linear.MLE$par
set.seed(123)
data2 <- data.frame(x = runif(200, 1, 10))
data2$y <- 0 + 2.75*data2$x + rnorm(200, 0, 1.5)
#creating the linear_likelihood function
linear_lik <- function(theta, y, X) {
n <- nrow(X)
k <- ncol(X)
beta <- theta[1:k]
sigma2 <- theta[k + 1]^2
e <- y - X %*% beta
logl <- -0.5 * n * log(2 * pi) - 0.5 * n * log(sigma2) - (t(e) %*% e) / (2 * sigma2)
return(-logl)
}
#using the optima function to create a linear_MLE function
#that uses the linear_lik function
linear_MLE <- optim(
fn = linear_lik,
par = c(1, 1, 1),
hessian = TRUE,
y = data2$y,
X = cbind(1, data2$x),
method = "BFGS"
)
linear_MLE$par
set.seed (123) # Setting my seed
data2 <- data.frame(x = runif(200, 1, 10)) # Creating new data
data2$y <- 0 + 2.75*data2$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize (or minimze its negative).
# My parameters for OLS are the intercept, the slope and the variance of the error term (sigma squared).
# So I take the formula of the log likelihood for a normal distribution.
# This is based on the slides for class:
linear.lik <- function(theta, y, X) {
n <- nrow(X) # No. obs
k <- ncol(X) # No. param
beta <- theta[1:k] # Regression coefs
sigma2 <- theta[k + 1]^2 # Squared variance
e <- y - X %*% beta # Calculating residuals
logl <- -0.5 * n * log(2 * pi) - 0.5 * n * log(sigma2) - ((t(e) %*% e) / (2 * sigma2))
return(-logl) # Returning negative log-likelihood for minimization
}
# Maximizing the log-likelihood (or minimizing negative log likelihood) using BFGS method.
# This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data2$y, X = cbind(1, data2$x), method = "BFGS")
# Seeing results:
linear.MLE$par
# Comparing with lm:
lm_mod <- lm(y ~ x, data = data2)
summary(lm_mod)
set.seed(123) # Setting my seed
data1 <- rcauchy(1000) # Creating my random variables
# Creating empirical distribution of observed data
ECDF <- ecdf(data1) # Creating my function
empiricalCDF <- ECDF(data1) # Using function to calculate cumulative probs for data
# Generating test statistic
D <- max(abs(empiricalCDF - pnorm(data1)))
print(D)
# Creating function to calculate p-value:
# I follow the given formula.
# I use sum from k = 1 to 1000, instead of infinity.
kol_smirn_p <- function(D) {
# Creating constants
sqrtpi = sqrt(2 * pi)
# Initialiting variables
sum_serie = 0
# Summing from k = 1 to 1000
for (k in 1:1000) {
term = exp(-(2*k - 1)^2 * pi^2 / (8 * D^2))
sum_serie = sum_serie + term
}
p_val = (sqrtpi / D) * sum_serie
return(p_val)
}
# Substiting our D test statistic value:
pvalue <- kol_smirn_p(D)
print(pvalue)
ks_result <- ks.test(data1, "pnorm", mean=mean(data1), sd=sd(data1))
summary(ks_result)
print(ks_result)
set.seed(123) # Setting my seed
data1 <- rcauchy(1000) # Creating my random variables
# Creating empirical distribution of observed data
ECDF <- ecdf(data1) # Creating my function
empiricalCDF <- ECDF(data1) # Using function to calculate cumulative probs for data
# Generating test statistic
D <- max(abs(empiricalCDF - pnorm(data1)))
print(D)
# Creating function to calculate p-value:
# I follow the given formula.
# I use sum from k = 1 to 1000, instead of infinity.
kol_smirn_p <- function(D) {
# Creating constants:
sqrtpi = sqrt(2 * pi)
# Initializing empty variables:
sum_serie = 0
# Summing from k = 1 to 1000, using a for loop:
for (k in 1:1000) {
term = exp(-(2*k - 1)^2 * pi^2 / (8 * D^2))
sum_serie = sum_serie + term
}
p_val = (sqrtpi / D) * sum_serie
return(p_val)
}
# Substituting our D test statistic value:
pvalue <- kol_smirn_p(D)
print(pvalue)
# Comparing to ks.test results:
ks_result <- ks.test(data1, "pnorm", mean=mean(data1), sd=sd(data1))
print(ks_result)
ks_comp <- ks.test(data1, "pnorm", mean=mean(data1), sd=sd(data1))
print(ks_comp)
ks_comp <- ks.test(data, pnorm)
print(ks_comp)
ks_comp <- ks.test(data1, pnorm)
print(ks_comp)
set.seed (123) # Setting my seed
data2 <- data.frame(x = runif(200, 1, 10)) # Creating new data
data2$y <- 0 + 2.75*data2$x + rnorm(200, 0, 1.5) # Modeling relationship
# Defining my log-likelihood function that I will maximize (or minimze its negative).
# My parameters for OLS are the intercept, the slope and the variance of the error term (sigma squared).
# So I take the formula of the log likelihood for a normal distribution.
# This is based on the slides for class:
linear.lik <- function(theta, y, X) {
n <- nrow(X) # No. obs
k <- ncol(X) # No. param
beta <- theta[1:k] # Regression coefs
sigma2 <- theta[k + 1]^2 # Squared variance
e <- y - X %*% beta # Calculating residuals
logl <- -0.5 * n * log(2 * pi) - 0.5 * n * log(sigma2) - ((t(e) %*% e) / (2 * sigma2))
return(-logl) # Returning negative log-likelihood for minimization
}
# Maximizing the log-likelihood (or minimizing negative log likelihood) using BFGS method.
# This code is also taken from slides:
linear.MLE <- optim(fn = linear.lik, par = c(1, 1, 1), hessian = TRUE, y = data2$y, X = cbind(1, data2$x), method = "BFGS")
# Seeing results:
linear.MLE$par
# Comparing with lm:
lm_mod <- lm(y ~ x, data = data2)
summary(lm_mod)
