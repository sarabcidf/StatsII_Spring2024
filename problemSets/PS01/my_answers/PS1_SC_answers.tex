\documentclass[12pt,letterpaper]{article}
\usepackage{graphicx,textcomp}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage{color}
\usepackage[reqno]{amsmath}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{amssymb,enumerate}
\usepackage[all]{xy}
\usepackage{endnotes}
\usepackage{lscape}
\newtheorem{com}{Comment}
\usepackage{float}
\usepackage{hyperref}
\newtheorem{lem} {Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{cor}{Corollary}
\newtheorem{obs}{Observation}
\usepackage[compact]{titlesec}
\usepackage{dcolumn}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usepackage{multirow}
\usepackage{xcolor}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{d}[1]{D{.}{.}{#1}}
\definecolor{light-gray}{gray}{0.65}
\usepackage{url}
\usepackage{listings}
\usepackage{color}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{magenta},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}
\lstset{style=mystyle}
\newcommand{\Sref}[1]{Section~\ref{#1}}
\newtheorem{hyp}{Hypothesis}

\title{Problem Set 1}
\date{Due: February 11, 2024}
\author{Applied Stats II}


\begin{document}
	\maketitle
	\section*{Instructions}
	\begin{itemize}
	\item Please show your work! You may lose points by simply writing in the answer. If the problem requires you to execute commands in \texttt{R}, please include the code you used to get your answers. Please also include the \texttt{.R} file that contains your code. If you are not sure if work needs to be shown for a particular problem, please ask.
\item Your homework should be submitted electronically on GitHub in \texttt{.pdf} form.
\item This problem set is due before 23:59 on Sunday February 11, 2024. No late assignments will be accepted.
	\end{itemize}

	\vspace{.25cm}
\section*{Question 1} 
\vspace{.25cm}
\noindent The Kolmogorov-Smirnov test uses cumulative distribution statistics test the similarity of the empirical distribution of some observed data and a specified PDF, and serves as a goodness of fit test. The test statistic is created by:

$$D = \max_{i=1:n} \Big\{ \frac{i}{n}  - F_{(i)}, F_{(i)} - \frac{i-1}{n} \Big\}$$

\noindent where $F$ is the theoretical cumulative distribution of the distribution being tested and $F_{(i)}$ is the $i$th ordered value. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all $x$ values. Large values indicate dissimilarity and the rejection of the hypothesis that the empirical distribution matches the queried theoretical distribution. The p-value is calculated from the Kolmogorov-
Smirnoff CDF:

$$p(D \leq d)= \frac{\sqrt {2\pi}}{d} \sum _{k=1}^{\infty }e^{-(2k-1)^{2}\pi ^{2}/(8d^{2})}$$


\noindent which generally requires approximation methods (see \href{https://core.ac.uk/download/pdf/25787785.pdf}{Marsaglia, Tsang, and Wang 2003}). This so-called non-parametric test (this label comes from the fact that the distribution of the test statistic does not depend on the distribution of the data being tested) performs poorly in small samples, but works well in a simulation environment. Write an \texttt{R} function that implements this test where the reference distribution is normal. Using \texttt{R} generate 1,000 Cauchy random variables (\texttt{rcauchy(1000, location = 0, scale = 1)}) and perform the test (remember, use the same seed, something like \texttt{set.seed(123)}, whenever you're generating your own data).\\
	
	
\noindent As a hint, you can create the empirical distribution and theoretical CDF using this code:

\begin{lstlisting}[language=R]
	# create empirical distribution of observed data
	ECDF <- ecdf(data)
	empiricalCDF <- ECDF(data)
	# generate test statistic
	D <- max(abs(empiricalCDF - pnorm(data))) \end{lstlisting}

\vspace{.2in}

\noindent \textbf{Answer to Question 1:}

\vspace{.1in}

\noindent First I set my seed, create my random variables and create my empirical distribution of observed data, following the code that is provided: 

\vspace{.1in}
 \lstinputlisting[language=R, linerange={30-36}]{PS1_SC_answers.R}
\vspace{.1in}
 
\noindent Then I can generate my test statistic with the code provided: 

\vspace{.1in}
 \lstinputlisting[language=R, linerange={37-40}]{PS1_SC_answers.R}
 \vspace{.1in}
 
\noindent The resulting test statistic is \textbf{0.1347281}. 

\noindent And I can create a function to calculate the corresponding p-value, following the provided formula: 

\vspace{.1in}
 \lstinputlisting[language=R, linerange={41-61}]{PS1_SC_answers.R}
\vspace{.1in}

\noindent Finally, substituting our test statistic in our function: 

\vspace{.1in}
 \lstinputlisting[language=R, linerange={62-65}]{PS1_SC_answers.R}
 \vspace{.1in}
 
\noindent  This yields a p-value of \textbf{5.652523e-29}. 

\noindent This is very close to the values produced by the ks.test() function, which yields \textbf{0.13573} for the test statistic and \textbf{2.22e-16} for the p-value. 

\noindent Code to generate results from ks.test(): 

\vspace{.1in}
 \lstinputlisting[language=R, linerange={66-69}]{PS1_SC_answers.R}
 \vspace{.1in}
 
\noindent We have used the Kolmogorov-Smirnov test to determine if a sample (the sample we created using rcauchy) comes from a population with a specific distribution (in this case, a normal distribution). 
\noindent From the very small p-value that we obtain (smaller than the standard 0.05, 0.01, or 0.001), we can reject the null hypothesis that the sample is drawn from a normal distribution, as it was expected. 


\newpage
\section*{Question 2}
\noindent Estimate an OLS regression in \texttt{R} that uses the Newton-Raphson algorithm (specifically \texttt{BFGS}, which is a quasi-Newton method), and show that you get the equivalent results to using \texttt{lm}. Use the code below to create your data.
\vspace{.5cm}
\lstinputlisting[language=R, firstline=51,lastline=53]{PS1.R} 

\vspace{.2in}

\noindent \textbf{Answer to Question 2:}

\vspace{.1in}

\noindent First I set my seed, create new data and a relationship, following the code provided: 

\vspace{.1in}
\lstinputlisting[language=R, linerange={72-75}]{PS1_SC_answers.R}
\vspace{.1in}

\noindent Then I define my log-likelihood function for optimization. This is based on slides from class: 

\vspace{.1in}
\lstinputlisting[language=R, linerange={76-91}]{PS1_SC_answers.R}
\vspace{.1in}

\noindent Then I can actually maximize my log-likelihood function (or minimize its negative, which is equivalent) with respect to my parameters. This code I also borrowed from slides: 

\vspace{.1in}
\lstinputlisting[language=R, linerange={92-95}]{PS1_SC_answers.R}
\vspace{.1in}

\noindent Looking at my results: 

\vspace{.1in}
\lstinputlisting[language=R, linerange={96-98}]{PS1_SC_answers.R}
\vspace{.1in}

\noindent This yields an estimated intercept of \textbf{0.1429829}, an estimated slope of \textbf{2.7263116}, and a n estimated sigma of \textbf{-1.4423360}. 

\noindent And finally, comparing with lm: 

\vspace{.1in}
\lstinputlisting[language=R, linerange={99-102}]{PS1_SC_answers.R}
\vspace{.1in}

\noindent Lm yields an estimated intercept of \textbf{0.13919}, and an estimated slope of \textbf{2.72670}, both of which are quite close to our maximum likelihood estimates. 

\noindent The results of ordinary least squares and maximum likelihood estimation should be approximately the same, especially as the size of the data gets larger, however they will not be exactly the same, as MLE follows a different estimation method which involves optimization. 

\end{document}
